# -*- coding: utf-8 -*-
"""college.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dbf75GVEgDdxSfY4xjUqCB-qzI5kj3On
"""

import pandas as pd
import numpy as np
from scipy.stats import zscore
from google.colab import drive
from sklearn.linear_model import LinearRegression as LR
from scipy.optimize import minimize
import re
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.feature_selection import SelectFromModel
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import make_pipeline
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import seaborn as sns
import matplotlib.pyplot as plt

drive.mount('/content/drive/')

US_NEWS_2020_PATH = "/content/drive/MyDrive/6.S079_Project/data/us_news_2020.csv"
WEIGHTS_2020_PATH = "/content/drive/MyDrive/6.S079_Project/data/2020_weights.csv"

act_percentiles_2020 = {36: 100, 35: 99, 34: 99, 33: 97, 32: 96, 31: 94, 30: 92, 29: 90, 28: 88, 27: 85, 26: 82, 25: 78, 24: 74, 23: 70, 22: 65, 21: 60, 20: 54, 19: 48, 18: 42, 17: 36, 16: 30, 15: 24, 14: 17, 13: 11, 12: 6, 11: 2, 10: 1, 9: 1, 8: 1, 7: 1, 6: 1, 5: 1, 4: 1, 3: 1, 2: 1, 1: 1}

sat_percentiles_2020 = {1600: 100, 1590: 100, 1580: 100, 1570: 100, 1560: 100, 1550: 99, 1540: 99, 1530: 99, 1520: 99, 1510: 98, 1500: 98, 1490: 98, 1480: 97, 1470: 97, 1460: 97, 1450: 96, 1440: 96, 1430: 95, 1420: 95, 1410: 94, 1400: 94, 1390: 93, 1380: 92, 1370: 92, 1360: 91, 1350: 90, 1340: 89, 1330: 89, 1320: 88, 1310: 87, 1300: 86, 1290: 85, 1280: 84, 1270: 83, 1260: 82, 1250: 81, 1240: 80, 1230: 78, 1220: 77, 1210: 76, 1200: 74, 1190: 73, 1180: 72, 1170: 70, 1160: 69, 1150: 67, 1140: 66, 1130: 64, 1120: 62, 1110: 61, 1100: 59, 1090: 57, 1080: 55, 1070: 54, 1060: 52, 1050: 50, 1040: 48, 1030: 46, 1020: 45, 1010: 43, 1000: 41, 990: 39, 980: 38, 970: 36, 960: 34, 950: 33, 940: 31, 930: 29, 920: 28, 910: 26, 900: 25, 890: 23, 880: 22, 870: 20, 860: 19, 850: 18, 840: 16, 830: 15, 820: 14, 810: 12, 800: 11, 790: 10, 780: 9, 770: 8, 760: 7, 750: 6, 740: 5, 730: 5, 720: 4, 710: 3, 700: 3, 690: 2, 680: 2, 670: 1, 660: 1, 650: 1, 640: 1, 630: 1, 620: 1, 610: 1, 600: 1}

us_news_2020 = pd.read_csv(US_NEWS_2020_PATH)
weights_2020 = pd.read_csv(WEIGHTS_2020_PATH)
display(us_news_2020)

clean_dict = {
    '2020 Rank': ['rank', 'int'],
    'School' : ['school', 'str'],
    'Overall Score': ['score', 'int'],
    'Peer Assessment Score': ['peer_score', 'float64'],
    'Graduation and retention rank': ['graduation_retention_rank', 'int'],
    'Over/Underperformance': ['graduation_diff', 'int'],
    'Social Mobility Rank': ['social_mobility_rank', 'int'],
    'Faculty resources rank': ['faculty_resources_rank', 'int'],
    'SAT/ACT 25th-75th percentile': ['test_scores', 'range'],
    'First year students in top 10% of high school class': ['accomplished_freshman', 'pct'],
    'Financial resources rank': ['financial_resources_rank', 'int'],
    'Average alumni giving rate': ['alum_giving_rate', 'pct'],
}

def ratio_converter(ratio_str):
  numerator, denominator = map(float, ratio_str.split(' to '))
  return numerator / denominator if denominator != 0 else None

def range_converter(range_str):
  if type(range_str) == str:
    bottom, top = map(float, range_str.split('-'))
    # converts the top and bottom range to the ACT/SAT Percentiles for the year 2020
    if top <= 36:
      bottom = act_percentiles_2020[bottom]
      top = act_percentiles_2020[top]
    else:
      bottom = sat_percentiles_2020[round(bottom, -1)]
      top = sat_percentiles_2020[round(top, -1)]
    return (bottom + top)/2
  else: # in case of 'nan' values
     return range_str

def rank_converter(df, col):
  max_rank = df[col].max()
  df[col] = (max_rank + 1 - df[col]) / max_rank
  return df

def clean_2020_data(df, trans_dic):
  for col in df.columns:
    try:
      _name, _type = trans_dic[col]
      df = df.rename(columns={col: _name})
      if _type in ['int', 'float64']:
        df[_name] = pd.to_numeric(df[_name], errors='coerce')
      elif _type == 'str':
        df[_name] = df[_name].astype('str')
      elif _type == 'pct':
        df[_name] = df[_name].str.rstrip('%').astype(float) / 100
      elif _type == 'ratio':
        df[_name] = df[_name].apply(ratio_converter)
      elif _type == 'range':
        df[_name] = df[_name].apply(range_converter)
    except:
      df = df.drop(columns=col)

  #==========================================
  # CONTEXT DEPENDENT (i.e 2020) TRANSFORMATIONS
  #==========================================
  # 'NaN' represents a difference of 0
  df['graduation_diff'] = df['graduation_diff'].fillna(0)

  # convert ranks
  df = rank_converter(df, 'social_mobility_rank')
  df = rank_converter(df, 'faculty_resources_rank')
  df = rank_converter(df, 'graduation_retention_rank')

  # apply z-score to standardize scores as described by U.S News
  df = pd.concat([df[['school', 'rank', 'score']], df.drop(columns=['score', 'school', 'rank']).dropna().apply(zscore)], axis=1)
  #==========================================

  return df

college_df = clean_2020_data(us_news_2020, clean_dict)
clean_2020_df = clean_2020_data(us_news_2020, clean_dict)
display(college_df)

"""**First Order analysis**: regress the weights. Not terrible, but not great either."""

def linear_regression(df):
  df = df.dropna()
  X = df.drop(columns=['school','rank', 'score'])
  y = df['score']
  model = LR()
  model.fit(X, y)
  print(X.columns)
  print("Coefficients:", model.coef_)
  print("Intercept:", model.intercept_)
  predictions = np.round(model.predict(X))
  df['pred_score'] = predictions
  df['pred_rank'] = df['pred_score'].rank(method="min", ascending=False)

  return df

lr_df = linear_regression(college_df)
display(lr_df[['school', 'rank', 'pred_rank', 'score', 'pred_score']])

"""**Second Order Analysis** Read into the USNCR using z-scores for each category, not simply calculating a score based off of a weighted average.

**From an older revision of Wikipedia:**




*The following are elements in the U.S. News rankings as of the 2020 edition.*



*   *Peer assessment: a survey of the institution's reputation among presidents, provosts, and admissions deans of other institutions (20%)*
*   *Retention: six-year graduation rate and first-year student retention rate (22%)*
*   *Social mobility: six-year graduation rates of students receiving Pell Grants—both as a standalone measure and compared to graduation rates of all other students at the school—adjusted significantly to give more credit to schools enrolling larger proportions of students receiving Pell Grants. (5%)*
*   *Faculty resources: class sizes, faculty salary, faculty degree level, student-faculty ratio, and proportion of full-time faculty (20%)*
*   *Student excellence: standardized test scores of admitted students and proportion of admitted students in upper percentiles of their high school class.* (unstated; by remainder --> 10%)
*   *Financial resources: per-student spending related to academics, student support and public service. (10%)*
*   *Graduation rate performance: comparison between modeled expected and actual graduation rate (8%)*
*   *Alumni giving rate (5%)*
"""

weights = {
    'peer_score': 0.2,
    'graduation_retention_rank': 0.22,
    'social_mobility_rank': 0.05,
    'faculty_resources_rank': 0.2,
    'graduation_diff': 0.08,
    'test_scores': 0.075,                # this and one below account for 'Student Excellence (10%)'
    'accomplished_freshman': 0.025,
    'financial_resources_rank': 0.1,
    'alum_giving_rate': 0.05,
    }

def min_max_scale(data, new_min, new_max):
    old_min = min(data)
    old_max = max(data)
    scaled_data = new_min + ((data - old_min) * (new_max - new_min) / (old_max - old_min))
    return scaled_data

def calculate_weighted_average_score(df):
  df['pred_score'] = 0
  for col, weight in weights.items():
    # print(f"{col}\n{df[col].dtype}\n")
    df['pred_score'] = df['pred_score'] + df[col] * weight
    # print(f"{df['pred_score']}\n\n")
  df['pred_score'] = df['pred_score'] / df['pred_score'].max() * 100
  df['pred_score'] = min_max_scale(df['pred_score'], 27, 100)
  df['pred_rank'] = df['pred_score'].rank(method="min", ascending=False)
  return df

weighted_avg_df = calculate_weighted_average_score(college_df)
display(weighted_avg_df[['school', 'rank', 'pred_rank', 'score', 'pred_score']])

top_lr_df = lr_df.head(10)
top_avg_df = weighted_avg_df.head(10)

for replication, _str in [[top_lr_df, 'Top10 LR'],
                          [top_avg_df, 'Top10 Avg'],
                          [lr_df.dropna(), 'All LR'],
                          [weighted_avg_df.dropna(), 'All Avg']]:
  print(f"{_str}: {mean_squared_error(replication['pred_score'], replication['score'])}")

def plot_scores_hbar(df):
  df = df.iloc[::-1].reset_index(drop=True)
  # Set the positions and width for the bars
  positions = list(range(len(df['score'])))
  height = 0.3  # the height of a bar

  fig, ax = plt.subplots(figsize=(5,5))  # Set up the plot size

  # Plotting the bars
  plt.barh([p - height for p in positions], df['score'], height, label='Actual Score', color='blue')
  plt.barh(positions, df['pred_score'], height, label='Predicted Score', color='orange')

  # Adding some aesthetics
  ax.set_title('Comparison of Actual and Predicted Scores by School')
  ax.set_ylabel('College')
  ax.set_xlabel('Score')
  ax.set_xticks([_*5 for _ in range(21)])
  ax.set_yticks([p for p in positions])
  ax.set_yticklabels(df['school'], )

  # Adding a legend
  plt.legend(loc=(1,0))

  # Show the plot
  plt.show()

def plot_scores_line(df):
  df = df.sort_values(by='rank')

  # Plotting the data
  fig, ax = plt.subplots(figsize=(25,5))  # Set up the plot size

  # Plotting the lines
  plt.plot(df['rank'], df['score'], marker='o', label='Actual Score', color='blue')
  plt.plot(df['rank'], df['pred_score'], marker='o', label='Predicted Score', color='orange')

  # Adding some aesthetics
  ax.set_title('Comparison of Actual and Predicted Scores by Rank')
  ax.set_xlabel('Rank')
  ax.set_ylabel('Score')
  plt.xticks(list(range(1, df['rank'].max(), 10)))  # Set x-ticks to be the ranks
  ax.set_xticklabels(list(range(1, df['rank'].max(), 10)), rotation=45)
  # Adding a legend
  plt.legend(loc='best')  # Automatically find the best location for the legend

  # Show the plot
  plt.show()

plot_scores_hbar(top_lr_df)
plot_scores_hbar(top_avg_df)

plot_scores_line(lr_df)
plot_scores_line(weighted_avg_df)

# prompt: First, drop the 'pred_score' column of weighted_avg_df and rename 'dist_score' column to 'pred_scoere'. Concatanate the first 10 rows of weighted_avg_df with everything besides the first 10 rows of linear_regression(clean_2020_df)

import pandas as pd
avg_df_head = weighted_avg_df.drop(columns=['pred_score'])
avg_df_head = avg_df_head.rename(columns={'dist_score': 'pred_score'})
combined_df = pd.concat([avg_df_head.head(10), linear_regression(clean_2020_df).iloc[10:]])

# display(combined_df[['school', 'rank', 'score', 'pred_score']])
# display(weighted_avg_df.drop(columns=['pred_score']).rename(columns={'dist_score': 'pred_score'})[['school', 'rank', 'score', 'pred_score']])

plot_scores_line(combined_df)

"""Try to use optimization function to determine weights based on matching rankings, initial assumptions, and objective function"""

df = clean_2020_df
def calculate_score(weights):
    weighted_scores = (df['peer_score'] * weights[0] +
                       df['graduation_retention_rank'] * weights[1] +
                       df['social_mobility_rank'] * weights[2] +
                       df['faculty_resources_rank'] * weights[3] +
                       df['graduation_diff'] * weights[4] +
                       df['test_scores'] * weights[5] +
                       df['accomplished_freshman'] * weights[6] +
                       df['financial_resources_rank'] * weights[7] +
                       df['alum_giving_rate'] * weights[8])
    return weighted_scores

def objective_function(weights):
    estimated_ranks = calculate_score(weights)
    rank_diff = (estimated_ranks - df['rank']).abs()  # Assuming 'rank' is the actual rank column
    return rank_diff.sum()

# Initial guesses for weights
initial_weights = np.array([0.2, 0.22, 0.05, 0.2, 0.08, 0.075, 0.025, 0.1, 0.05])
initial_weights_zero = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0])


# constraint is that the sum of weights is 1
constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})

bounds = [(0, 1)] * 9

result = minimize(objective_function, initial_weights_zero, method='SLSQP', bounds=bounds, constraints=constraints)

if result.success:
    fitted_weights = result.x
    formatted_weights = [f"{weight:.6f}" for weight in fitted_weights]

    print("Optimal weights:", formatted_weights)
    df['op_score'] = calculate_score(fitted_weights)
    df['op_rank'] = df['op_score'].rank(method="min")
    display(df[['school', 'rank', 'op_rank', 'score', 'op_score']])
else:
    print("No solution found.")



rank_df = clean_2020_df
def calculate_score(weights):
    weighted_scores = (rank_df['peer_score'] * initial_weights[0] +
                       rank_df['graduation_retention_rank'] * weights[0] +
                       rank_df['social_mobility_rank'] * weights[1] +
                       rank_df['faculty_resources_rank'] * weights[2] +
                       rank_df['graduation_diff'] * initial_weights[4] +
                       rank_df['test_scores'] * initial_weights[5] +
                       rank_df['accomplished_freshman'] * initial_weights[6] +
                       rank_df['financial_resources_rank'] * weights[3] +
                       rank_df['alum_giving_rate'] * initial_weights[8])
    return weighted_scores

# Objective function minimizes the difference from actual ranks
def objective_function(weights):
    estimated_ranks = calculate_score(weights)
    rank_diff = (estimated_ranks - df['rank']).abs()
    return rank_diff.sum()

# Initial guesses for weights
initial_weights = np.array([0.2, 0.22, 0.05, 0.2, 0.08, 0.075, 0.025, 0.1, 0.05])
# graduation_retention_rank, social_mobility_rank, faculty_resources_rank, financial_resources_rank
initial_weights_zero = np.array([0, 0, 0, 0])
initial_rank_weights = np.array([0.22, 0.05, 0.2, 0.1])



# constraint is that the sum of weights is 1
constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})

# Bounds for weights, ensuring all weights are between 0 and 1
bounds = [(0, 1)] * 4

result = minimize(objective_function, initial_rank_weights, method='SLSQP', bounds=bounds, constraints=constraints)

if result.success:
    fitted_weights = result.x
    formatted_weights = [f"{weight:.6f}" for weight in fitted_weights]

    print("Optimal weights:", formatted_weights)
else:
    print("No solution found.")

features = rank_df[['peer_score', 'faculty_resources_rank', 'social_mobility_rank', 'financial_resources_rank']]
features = features.dropna()

# normalize features
for col in features.columns:
    features[col] = (features[col] - features[col].mean()) / features[col].std()

kmeans = KMeans(n_clusters=5, random_state=42).fit(features)
features['cluster'] = kmeans.labels_

sns.pairplot(features, hue='cluster', palette='bright')
plt.show()

"""Some interesting correlations here:
- peer score and financial resources rank seem to be highly correlated
- faulty resources and financial resources also see roughly correlated
- high densities for groups 0 and 4

Cluster 0: schools with generally lower ranks across the features due to the spread and central tendency of points in the lower range.

Cluster 1: schools that perform well in terms of peer score but variably in other features.

Cluster 2: schools that have moderate to high rankings across all features.

Cluster 3: schools with high social mobility ranks but mixed in other areas.

Cluster 4:  high faculty resources and financial resources ranks but lower in peer scores and social mobility.
"""

# reduce dimensionality of dataset seems like there are groups of universities with common characteristics
pca = PCA(n_components=2)
principalComponents = pca.fit_transform(features)
principalDf = pd.DataFrame(data=principalComponents, columns=['principal component 1', 'principal component 2'])

plt.figure(figsize=(8,6))
sns.scatterplot(x='principal component 1', y='principal component 2', data=principalDf)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA of University Ranking Features')
plt.show()

# create a model to try and predict the ranking of a given unseen school

X = rank_df.drop(['school', 'rank', 'score'], axis=1)
y = rank_df['score']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

imputer = SimpleImputer(strategy='mean')

model = RandomForestRegressor(n_estimators=100, random_state=42)

# pipeline will first impute missing values then fit the model
pipeline = make_pipeline(imputer, model)

pipeline.fit(X_train, y_train)

predictions = pipeline.predict(X_test)
print("Predictions:", predictions)

mse = mean_squared_error(y_test, predictions)
print("MSE:", mse)

"""Look at schools that have changed a lot and dig into why
sensitivity

"""

# Preparing the data
X = rank_df.drop(['school', 'rank', 'score'], axis=1)
y = rank_df['score']

# Splitting data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Creating an imputer object to fill missing values
imputer = SimpleImputer(strategy='median')  # Change strategy to 'median' for a different approach

# Creating a RandomForestRegressor model with hyperparameter tuning
model = RandomForestRegressor(n_estimators=200, max_depth=10, min_samples_split=5, min_samples_leaf=2, random_state=42)

# Creating a pipeline that first imputes missing values then fits the model
pipeline = Pipeline([
    ('imputer', imputer),
    ('feature_selection', SelectFromModel(RandomForestRegressor(n_estimators=100))),
    ('regression', model)
])

# Grid Search for parameter tuning
param_grid = {
    'regression__n_estimators': [100, 200],
    'regression__max_depth': [None, 10, 20],
    'regression__min_samples_leaf': [1, 2, 4],
    'regression__min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)
best_model = grid_search.best_estimator_

# Predicting on the test data
predictions = best_model.predict(X_test)
print("Predictions:", predictions)

# Calculating Mean Squared Error
mse = mean_squared_error(y_test, predictions)
print("MSE:", mse)

"""Sophie - Ranking Sensitivity

"""

HISTORICAL_RANKINGS_PATH = "/content/drive/MyDrive/6.S079_Project/data/us_news_rankings_2019_2024.csv"
rankings = pd.read_csv(HISTORICAL_RANKINGS_PATH)
display(rankings)

def handle_rankings(rank):
    if pd.isna(rank):
        return None
    if isinstance(rank, str):
        # Use regular expressions to remove "T" followed by any digits
        rank = re.sub(r'T\d+', '', rank)
        if '-' in rank:
            low, high = rank.replace('(', '').replace(')', '').split('-')
            return (int(low) + int(high)) // 2 # replace ranges with medium number in the range
    return int(rank)

# Apply the function to all year columns
for year in ['2024', '2023', '2022', '2021', '2020', '2019']:
    rankings[year] = rankings[year].apply(handle_rankings)

# drop columns that don't have rankings for at least 2 of the years to compare
rankings['non_nan_count'] = rankings[['2019', '2020', '2021', '2022', '2023', '2024']].apply(lambda row: row.count(), axis=1)
df_filtered = rankings[rankings['non_nan_count'] >= 2]
df_filtered = df_filtered.drop(columns=['non_nan_count'])

# Print the filtered DataFrame
print(df_filtered)

# display(rankings)

# Calculate changes in rankings
df_filtered['Max Change'] = df_filtered.apply(
    lambda row: max(
        [abs(row[year] - row[str(int(year) + 1)])
         for year in ['2019', '2020', '2021', '2022']
         if not pd.isna(row[year]) and not pd.isna(row[str(int(year) + 1)])
        ], default=0  # Provide a default value here
    ), axis=1
)
# # Sort by the maximum change
df_sorted = df_filtered.sort_values(by='Max Change', ascending=False)

# # Print the top colleges with the largest changes
print(df_sorted[['College Name', 'State', '2019', '2020', '2021', '2022', '2023', '2024', 'Max Change']].head())